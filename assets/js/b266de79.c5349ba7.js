"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[3518],{4369(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2026/01/25/jekyll-to-docusaurus","metadata":{"permalink":"/2026/01/25/jekyll-to-docusaurus","source":"@site/blog/2026-01-25-jekyll-to-docusaurus.md","title":"From Jekyll to Docusaurus","description":"Why I moved away from a Jekyll-based GitHub Pages site to Docusaurus, how Claude helped me get there, and the small gotcha that caused a 404 on launch day.","date":"2026-01-25T00:00:00.000Z","tags":[{"inline":true,"label":"Docusaurus","permalink":"/tags/docusaurus"},{"inline":true,"label":"Jekyll","permalink":"/tags/jekyll"},{"inline":true,"label":"personal project","permalink":"/tags/personal-project"},{"inline":true,"label":"Gen AI","permalink":"/tags/gen-ai"}],"readingTime":3.42,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"From Jekyll to Docusaurus","description":"Why I moved away from a Jekyll-based GitHub Pages site to Docusaurus, how Claude helped me get there, and the small gotcha that caused a 404 on launch day.","tags":["Docusaurus","Jekyll","personal project","Gen AI"]},"unlisted":false,"nextItem":{"title":"Understanding AI Agents","permalink":"/2026/01/10/aiagents"}},"content":"I\'d been running this blog on a Jekyll theme for a while. It worked, but I wanted to change the theme and the layput and it was not easy. The friction was real.\\n\\nI had heard about Docusaurus - React-based, clean defaults, built for content. I was curious enough to try it.\\n\\nHere\'s the thing - I didn\'t want to spend days on migration. I wanted a working site in a weekend. So I brought Claude into the process from the start.\\n\\n**Getting the Environment Ready**\\n\\nFirst step: Node. Docusaurus requires Node 18 or higher. I already had it installed, but if you\'re starting from scratch, the [Node.js site](https://nodejs.org) has straightforward installers for Windows, Mac, and Linux.\\n\\nOnce Node was in place, scaffolding the project took one command:\\n\\n```bash\\nnpx create-docusaurus@latest my-blog classic\\n```\\n\\nThis sets up everything \u2014 the folder structure, default blog, config file, and a local dev server. I chose the `classic` preset because it includes the blog plugin out of the box, which is all I needed.\\n\\n**Where Claude Came In**\\n\\nI asked Claude to walk me through the migration. I explained what I had (a Jekyll site with a handful of blog posts and a few pages) and what I wanted (a Docusaurus blog with the same content, without docs or a landing page).\\n\\nThe back-and-forth was genuinely useful. Claude flagged things I hadn\'t thought about \u2014 like turning off the docs plugin entirely, setting up the blog as the homepage, and making sure the frontmatter format matched what Docusaurus expected.\\n\\nI didn\'t follow every suggestion blindly. Some recommendations needed adjusting once I saw how my specific setup worked. But having that starting point saved me from reading through the entire Docusaurus docs before writing a single line of config.\\n\\n**Migrating the Posts**\\n\\nI didn\'t move everything at once. I picked one post, copied it into the `blog/` folder, adjusted the frontmatter, and ran the local server:\\n\\n```bash\\nnpm start\\n```\\n\\nThe post showed up. The formatting looked right. That small proof of concept gave me the confidence to move the rest.\\n\\nThe main frontmatter difference from Jekyll: Docusaurus uses `tags` as an array directly in YAML, and it\'s stricter about dates matching the filename. I moved the remaining five posts in one go, ran the server again, and everything rendered cleanly.\\n\\n\\n**Setting Up Deployment**\\n\\nFor GitHub Pages, I created a GitHub Actions workflow at `.github/workflows/deploy.yml`. The workflow runs on every push to `main`, builds the site inside the `my-blog/` folder, and pushes the output to a `gh-pages` branch using the `peaceiris/actions-gh-pages` action.\\n\\nI committed everything and pushed.\\n\\n**The Gotcha**\\n\\nThe workflow ran successfully. The `gh-pages` branch was created. But the site showed a 404.\\n\\nTurned out the GitHub Pages source was still pointing to the `main` branch \u2014 left over from the old Jekyll setup. The fix took thirty seconds: **Settings \u2192 Pages \u2192 Source \u2192 branch: gh-pages, folder: /**\\n\\nThat was it. The site came up immediately after saving.\\n\\n**Styling Tweaks**\\n\\nOnce the content was live, I made a few small adjustments:\\n\\n- Replaced the default green color scheme with blue f \u2014 one CSS variable change that cascades through titles, tags, and the active nav tab\\n- Removed the Docusaurus logo \\n- Added `description` frontmatter to all posts for proper meta descriptions in search results\\n\\n\\nEach change was small, but together they made the site feel like mine rather than a starter template.\\n\\n**Was It Worth It?**\\n\\nYes. I liked the layout of Docusaurus more and it is node based. \\n\\nThe migration took a couple of hours. Most of that time was configuration and testing, not content work. The content moved cleanly because Markdown is Markdown.\\n\\nIf your current blog setup feels like a black box and you find yourself avoiding changes because you don\'t know what will break, it might be time for a rebuild. Start with one post, run it locally, see how it feels.\\n\\nSometimes the best reason to migrate is just that you\'ll actually enjoy maintaining it afterward.\\n\\n---\\n\\n*Built with Docusaurus. Migrated with Claude\'s help. Deployed on GitHub Pages.*"},{"id":"/2026/01/10/aiagents","metadata":{"permalink":"/2026/01/10/aiagents","source":"@site/blog/2026-01-10-aiagents.md","title":"Understanding AI Agents","description":"A practical explainer on AI agents \u2014 how they differ from regular chat AI, how the agent loop works, and what it means for teams starting to adopt them.","date":"2026-01-10T00:00:00.000Z","tags":[{"inline":true,"label":"Gen AI","permalink":"/tags/gen-ai"},{"inline":true,"label":"AI Agents","permalink":"/tags/ai-agents"},{"inline":true,"label":"Technical Writing","permalink":"/tags/technical-writing"}],"readingTime":5.52,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Understanding AI Agents","description":"A practical explainer on AI agents \u2014 how they differ from regular chat AI, how the agent loop works, and what it means for teams starting to adopt them.","tags":["Gen AI","AI Agents","Technical Writing"]},"unlisted":false,"prevItem":{"title":"From Jekyll to Docusaurus","permalink":"/2026/01/25/jekyll-to-docusaurus"},"nextItem":{"title":"Machine Learning Practice Problems: Classification vs Regression","permalink":"/2025/11/13/ml-classification-vs-regression"}},"content":"I\'ve been hearing a lot about AI agents - what they are, how they differ from regular ChatGPT/Copilot. Here\'s what I\'ve learned from working with them.\\n\\n## What Actually Is an AI Agent?\\n\\nHere\'s the thing - when you use ChatGPT or Copilot, you ask a question, get an answer, then ask another question. It\'s a conversation. You\'re in control of every step.\\n\\nAI agents flip this dynamic. You give them a goal, and they figure out how to get there. They don\'t wait for you to tell them what to do next - they plan, execute, check results, adjust, and keep going until the job is done.\\n\\nThink of it like the difference between giving someone turn-by-turn directions versus just telling them the destination. One requires constant input from you; the other handles navigation independently.\\n\\n\\n## The Agent Loop - How It Works\\n\\nBehind the scenes, AI agents run what\'s called an \\"agent loop.\\" It\'s basically a cycle that repeats until your task is complete:\\n\\n- Remember what\'s happened so far\\n- Decide what to do next\\n- Execute the action\\n- Check the result\\n- Update the memory with what worked (or didn\'t)\\n- Decide whether to continue or stop\\n\\nIt\'s how you\'d work through a complex project yourself - except the agent does this in minutes instead of hours.\\n\\n## What Makes Up an AI Agent?\\n\\n**Memory**  \\nThe agent\'s record of what\'s happened. It tracks your request, what actions it\'s taken, what worked, what failed. Without memory, the agent would repeat mistakes or lose track of progress.\\n\\n**Rules**  \\nThese define what the agent can and cannot do. Think of rules as the agent\'s job description - its boundaries, capabilities, and guidelines for behavior.\\n\\n**Tools**  \\nThe actual functions the agent can perform. Read files. Query databases. Send emails. Search the web. Create reports. Without tools, the agent can only talk about doing things - it can\'t actually do them.\\n\\n## Regular AI vs. Agentic AI\\n\\n**Regular Generative AI:**\\n- You: \\"Write a product description\\"\\n- AI: [writes description]\\n- You: \\"Make it shorter\\"\\n- AI: [makes it shorter]\\n- You: \\"Check it against brand voice\\"\\n- AI: [checks it]\\n\\nYou\'re driving every step.\\n\\n**Agentic AI:**\\n- You: \\"Create a product description that matches our brand voice and is under 150 words\\"\\n- AI: \\"Let me check your brand guidelines... drafting... checking word count... making adjustments... Done. Here\'s the description.\\"\\n\\nThe AI is driving.\\n\\n## What Makes Agentic AI Different\\n\\n**Flipped Interaction**  \\nInstead of you asking 20 questions, the AI might ask you 2-3 clarifying questions upfront, then goes off and completes the work.\\n\\n**Adaptive Planning**  \\nHits a roadblock? The agent adjusts its approach. Missing information? It searches for it or asks you. Error in step 3? It modifies steps 4 and 5 accordingly.\\n\\n**Translation Layer**  \\nYou say: \\"Pull last quarter\'s support tickets and identify common issues.\\"  \\nThe agent translates that into database queries, data analysis, pattern recognition, and report generation.  \\nYou get: A summary without writing code or manually reviewing hundreds of tickets.\\n\\n## Real Example - Reviewing UI Literals\\n\\nOne task that comes up regularly in documentation work is reviewing UI literals - the text strings that appear in software interfaces like button labels, error messages, and menu items.\\n\\n**The Old Way:**  \\nThe developers sends over a spreadsheet with new UI literals for an upcoming release. The writer needs to:\\n- Check each literal against our style guide rules (capitalization, punctuation, word choice)\\n- Get clarity from the product team on the context and requirements\\n- Comes from multiple developers, in sprints\\n\\nThis takes several hours of focused, tedious work. Miss one inconsistency, and it ships to users.\\n\\n**With an AI Agent:**  \\nGive the agent: \\"Review these 200 new literals against our style guide rules and our verified literal list. Flag anything that doesn\'t match patterns or rules.\\"\\n\\nThe agent:\\n- Loads the style guide rules (title casing, no periods in labels, specific terminology requirements)\\n- Compares each new literal against the guidelines\\n- Identifies 23 literals that don\'t follow capitalization rules\\n- Flags 8 literals using terminology that differs from established patterns (like \\"Remove\\" vs standard \\"Delete\\")\\n- Creates a report categorizing issues by type with specific recommendations\\n\\nThe writer reviews the flagged items, gets clarity from the product team, and does the needful. The pattern-matching work? Handled.\\n\\nWhat makes this powerful is consistency. The agent doesn\'t get tired on literal 150 and start missing things. It applies the same rigor to every single item.\\n\\n## Types of AI Agents\\n\\n**Simple Reflex Agents**  \\nFollow basic \\"if this, then that\\" rules. If someone asks about pricing, show the pricing page. Straightforward but limited.\\n\\n**Goal-Based Agents**  \\nWork toward specific objectives. \\"Identify the inconsistencies in the terminologies used, and provide steps to fix the,\\" The agent figures out how to achieve that goal.\\n\\n**Learning Agents**  \\nGet better over time. Based on user interactions and instructions, the responses changes overtime.\\n\\n## The Reality Check\\n\\nLet me be clear - AI agents aren\'t magic.\\n\\n**You\'re Still the Boss**  \\nSet the goals. Define the rules. Review the results. The agent executes; you provide strategy and judgment.\\n\\n**Start Small**  \\nTest with low-risk tasks first. Don\'t hand over mission-critical work until you\'ve seen how the agent performs on simpler tasks.\\n\\n**Oversight Matters**  \\nAgents make mistakes. They can go off track. You need to review their work, especially initially.\\n\\n**Human Judgment Can\'t Be Replaced**  \\nUse agents for execution and coordination. Keep strategic thinking, ethical considerations, and creative decisions in human hands.\\n\\n## What This Means for Your Team\\n\\n**More Time for Strategy**  \\nYour team spends less time on repetitive tasks and more time on work that actually needs human creativity and judgment.\\n\\n**Faster Execution**  \\nThings that took days now take hours. Give the task with proper guidelines and let the agent handle it.\\n\\n**Consistency**  \\nThe agent follows rules every time. No shortcuts. No \\"I\'ll fix that later\\" moments. It does what it\'s supposed to do.\\n\\n\\n## Getting Started\\n\\nIdentify one repetitive, multi-step task that eats up your team\'s time. That\'s your first candidate for an AI agent.\\n\\nIt might be:\\n- Generating status reports from multiple data sources\\n- Reviewing and categorizing customer feedback\\n- Creating first drafts of proposals from templates\\n- Monitoring compliance across multiple systems\\n\\nPick something where the process is clear, the inputs are structured, and the stakes aren\'t too high if something goes wrong.\\n\\nTest it. Refine it. Expand from there.\\n\\n## The Bottom Line\\n\\nAI agents aren\'t just fancy chatbots. They\'re systems that can handle complex, multi-step work with minimal hand-holding.\\n\\nThe technology is here. It\'s getting better fast. Companies figuring out how to use it effectively will have a competitive advantage.\\n\\nThe question isn\'t whether AI agents will change how we work - they already are. The question is: How will you put them to work on your team?"},{"id":"/2025/11/13/ml-classification-vs-regression","metadata":{"permalink":"/2025/11/13/ml-classification-vs-regression","source":"@site/blog/2025-11-13-ml-classification-vs-regression.md","title":"Machine Learning Practice Problems: Classification vs Regression","description":"A practical walkthrough of ML problems with both categorical and numerical targets \u2014 what changes in your pipeline, how to encode correctly, which metrics to use, and the gotchas to watch out for.","date":"2025-11-13T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"machine learning","permalink":"/tags/machine-learning"},{"inline":true,"label":"learning","permalink":"/tags/learning"}],"readingTime":5.92,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Machine Learning Practice Problems: Classification vs Regression","description":"A practical walkthrough of ML problems with both categorical and numerical targets \u2014 what changes in your pipeline, how to encode correctly, which metrics to use, and the gotchas to watch out for.","tags":["python","machine learning","learning"]},"unlisted":false,"prevItem":{"title":"Understanding AI Agents","permalink":"/2026/01/10/aiagents"},"nextItem":{"title":"Supervised Learning: A Quick Reference","permalink":"/2025/10/28/supervised-learning"}},"content":"When you start working through ML practice problems, you\'ll notice that two problems can look almost identical \u2014 same preprocessing steps, same train/test split structure \u2014 but one has a price as the target and the other has a category. That difference changes several things in your pipeline.\\n\\nThis post walks through real scenarios from both sides and highlights what to watch out for at each step.\\n\\n---\\n\\n## First: How to Tell What You\'re Working With\\n\\nBefore writing a single line of model code, check your target column:\\n\\n```python\\nprint(df[\'Target\'].dtype)          # object/category \u2192 Classification\\n                                    # int/float \u2192 likely Regression\\nprint(df[\'Target\'].nunique())       # few unique values \u2192 Classification\\nprint(df[\'Target\'].value_counts())  # see the class distribution\\n```\\n\\n**Regression targets** in the practice problems: `Price` (laptops, used cars) \u2014 a continuous number.\\n\\n**Classification targets** in the practice problems: `Churn` (Yes/No), `elective` (DataScience/WebDev/AI...), `Purchase_Category`, `Location`, `Flavor`, `Main_Course` \u2014 all category labels.\\n\\n---\\n\\n## What Changes Between the Two\\n\\n| Step | Regression | Classification |\\n|------|-----------|----------------|\\n| Target encoding | Nothing \u2014 it\'s already numeric | Encode labels to numbers |\\n| Model imports | `RandomForestRegressor`, `SVR`, etc. | `RandomForestClassifier`, `LogisticRegression`, etc. |\\n| Evaluation metrics | R\xb2, MAE, RMSE | Accuracy, Precision, Recall, F1 |\\n| Predicting | Returns a number | Returns a class label |\\n| Decoding predictions | Not needed | May need `inverse_transform()` to get label back |\\n\\nFeature preprocessing (missing values, encoding, scaling) is the same regardless. The differences kick in from the model choice onwards.\\n\\n---\\n\\n## Regression Scenarios\\n\\n### Laptop Price Prediction \u2014 Random Forest Regressor\\n\\nTarget: `Price` (a number \u2014 predict what a laptop costs)\\n\\n**Outlier handling before scaling:**\\n\\nA step you won\'t always see but matters here \u2014 the laptop dataset uses IQR to filter out outlier RAM values before training:\\n\\n```python\\nQ1 = df[\'RAM_GB\'].quantile(0.25)\\nQ3 = df[\'RAM_GB\'].quantile(0.75)\\nIQR = Q3 - Q1\\n\\nlower = Q1 - 1.5 * IQR\\nupper = Q3 + 1.5 * IQR\\n\\ndf = df[(df[\'RAM_GB\'] >= lower) & (df[\'RAM_GB\'] <= upper)]\\n```\\n\\nOutliers can skew a regressor\'s predictions significantly \u2014 a laptop with 1TB RAM as a data entry error will pull predictions up. Filter first, then scale.\\n\\n**Reserving rows for prediction:**\\n\\n```python\\npred = df.tail(5).copy()      # held-out rows for final prediction\\ndata = df.iloc[:-5].copy()    # everything else for train/test\\n```\\n\\nThis simulates new unseen data arriving \u2014 you train on everything up to those rows, then predict the last 5.\\n\\n**Evaluation:**\\n\\n```python\\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\\nimport numpy as np\\n\\nprint(\\"R2:\\", round(r2_score(y_test, y_pred), 3))\\nprint(\\"MAE:\\", round(mean_absolute_error(y_test, y_pred), 3))\\nprint(\\"RMSE:\\", round(np.sqrt(mean_squared_error(y_test, y_pred)), 3))\\n```\\n\\n- **R\xb2** tells you how much of the price variance your model explains. 0.85 means 85% explained \u2014 the higher the better.\\n- **MAE** tells you the average error in the same unit as your target (e.g., dollars). Easy to interpret.\\n- **RMSE** penalises large errors more heavily than MAE. If RMSE is much higher than MAE, your model has a few big misses worth investigating.\\n\\n---\\n\\n### Used Car Prices \u2014 Comparing Multiple Models\\n\\nSame regression setup but this problem goes a step further: it trains four models on the same data and compares them.\\n\\n**Feature engineering the target-related column:**\\n\\n```python\\ndf[\'Car_Age\'] = 2024 - df[\'Year\']\\n```\\n\\nThe raw `Year` column (e.g., 2015, 2018) is less meaningful to a model than how old the car actually is. Always ask: is the raw value what matters, or is a derived version more informative?\\n\\n**Comparing models side by side:**\\n\\n```python\\nmodels = {\\n    \'Random Forest\': RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42),\\n    \'KNN\':           KNeighborsRegressor(n_neighbors=7),\\n    \'Decision Tree\': DecisionTreeRegressor(max_depth=10, random_state=42),\\n    \'SVR\':           SVR(kernel=\'rbf\', C=100, gamma=0.1)\\n}\\n\\nfor name, model in models.items():\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    print(f\\"{name} \u2192 R2: {r2_score(y_test, y_pred):.3f} | \\"\\n          f\\"MAE: {mean_absolute_error(y_test, y_pred):.3f} | \\"\\n          f\\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}\\")\\n```\\n\\n**Watch out:** KNN and SVR require scaled features \u2014 they use distance calculations. Random Forest and Decision Tree do not require scaling. In this problem, scaling is applied to all models before any of them are trained, so all four get the same prepared data.\\n\\n---\\n\\n## Classification Scenarios\\n\\n### Customer Churn \u2014 Binary Classification\\n\\nTarget: `Churn` \u2014 Yes or No. Two classes.\\n\\n**Encoding the target:**\\n\\nThe `Churn` column contains string values (`\'Yes\'`/`\'No\'`). Most classifiers accept string targets directly, but being explicit is better:\\n\\n```python\\ndf[\'Churn\'] = df[\'Churn\'].map({\'Yes\': 1, \'No\': 0})\\n```\\n\\n**Encoding the features \u2014 LabelEncoder for tree-based models:**\\n\\n```python\\ncatg_cols = [\'Gender\', \'ContractType\', \'InternetService\', \'PaymentMethod\']\\nencoder = LabelEncoder()\\nfor col in catg_cols:\\n    df[col] = encoder.fit_transform(df[col])\\n```\\n\\nLabelEncoder assigns integers (0, 1, 2...) to categories. This is fine for Random Forest because trees split on thresholds \u2014 the numeric ordering doesn\'t imply a false relationship. For linear or distance-based models, use `pd.get_dummies()` instead.\\n\\n**Scale after reserving prediction rows:**\\n\\n```python\\ndf_pred = df.tail(3).copy()   # reserve before scaling\\ndf = df.iloc[:-3].copy()\\n\\nscaler = StandardScaler()\\ndf[num_cols] = scaler.fit_transform(df[num_cols])\\ndf_pred[num_cols] = scaler.transform(df_pred[num_cols])  # transform only, not fit\\n```\\n\\nThis is a critical gotcha \u2014 `fit_transform` on training data, `transform` only on prediction/test data. If you fit the scaler again on the prediction rows, you\'re computing different statistics and the scaling becomes inconsistent.\\n\\n**Evaluation:**\\n\\n```python\\nfrom sklearn.metrics import accuracy_score, precision_score, classification_report\\n\\nprint(\\"Accuracy:\\", round(accuracy_score(y_test, y_pred), 3))\\nprint(\\"Precision:\\", round(precision_score(y_test, y_pred), 3))\\nprint(classification_report(y_test, y_pred))\\n```\\n\\nFor churn, **Recall matters more than Precision** \u2014 missing a customer who is about to churn (False Negative) is more costly than flagging one who wasn\'t going to (False Positive). Accuracy alone can mislead you if most customers don\'t churn.\\n\\n---\\n\\n### Student Course Selection \u2014 Multi-Class Classification\\n\\nTarget: `elective` \u2014 one of six courses (DataScience, WebDev, AI, CloudComputing, Cybersecurity, GameDev).\\n\\nThis is where the encoding approach diverges from the binary case.\\n\\n**One-hot encoding both features AND target:**\\n\\n```python\\nX_en = pd.get_dummies(X, drop_first=True)   # features: drop_first to avoid multicollinearity\\ny_en = pd.get_dummies(y)                     # target: do NOT drop_first \u2014 you need all class columns\\n```\\n\\nNote: `drop_first=True` on the features, but NOT on the target. If you drop one class from the target, you can\'t recover which course was predicted.\\n\\n**Aligning prediction rows to training columns:**\\n\\n```python\\ntest_en = pd.get_dummies(test, drop_first=True)\\ntest_en = test_en.reindex(columns=X_en.columns, fill_value=0)\\n```\\n\\nWhen you one-hot encode a single prediction row, it may not contain all the categories that appeared in training \u2014 so some columns will be missing. `reindex` fills them with 0, ensuring the prediction row has exactly the same shape as what the model was trained on.\\n\\n**Decoding the prediction:**\\n\\n```python\\npred = model.predict(test_en)\\npred_course = y_en.columns[pred.argmax()]\\nprint(pred_course)  # e.g., \'DataScience\'\\n```\\n\\n`predict()` here returns a one-hot style array \u2014 `argmax()` finds the column (class) with the highest value, and `y_en.columns` maps it back to the course name.\\n\\n---\\n\\n## Key Gotchas Summary\\n\\n| Situation | What to Watch Out For |\\n|-----------|----------------------|\\n| Binary target (Yes/No) | Map to 1/0 explicitly \u2014 don\'t rely on alphabetical encoding |\\n| Multi-class target | One-hot encode without `drop_first` \u2014 you need every class column |\\n| LabelEncoder on features | Fine for tree models, not for linear/distance models |\\n| Scaling with reserved prediction rows | `fit_transform` on train data, `transform` only on prediction rows |\\n| Prediction row has unseen categories | Use `reindex(columns=X_train.columns, fill_value=0)` after get_dummies |\\n| Accuracy on imbalanced classes | Use `classification_report()` \u2014 read per-class Precision, Recall, F1 |\\n| Outliers before regression | Check with IQR before scaling \u2014 one extreme value skews the whole scaler |\\n| Feature engineering | Derive meaningful columns before training (e.g., Car_Age = 2024 - Year) |"},{"id":"/2025/10/28/supervised-learning","metadata":{"permalink":"/2025/10/28/supervised-learning","source":"@site/blog/2025-10-28-supervised-learning.md","title":"Supervised Learning: A Quick Reference","description":"A concise quick reference for Supervised Learning, covering classification vs regression, evaluation metrics, top models, and how to choose the right approach.","date":"2025-10-28T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"machine learning","permalink":"/tags/machine-learning"},{"inline":true,"label":"learning","permalink":"/tags/learning"}],"readingTime":3.69,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Supervised Learning: A Quick Reference","description":"A concise quick reference for Supervised Learning, covering classification vs regression, evaluation metrics, top models, and how to choose the right approach.","tags":["python","machine learning","learning"]},"unlisted":false,"prevItem":{"title":"Machine Learning Practice Problems: Classification vs Regression","permalink":"/2025/11/13/ml-classification-vs-regression"},"nextItem":{"title":"My Daily Workflow with Enterprise Copilot: A Technical Writer\'s Field Notes","permalink":"/2025/10/02/copilitusage"}},"content":"Supervised learning is a machine learning approach where models learn from labeled training data to make predictions on new, unseen data. The algorithm learns the relationship between input features (X) and target variables (y) to generalize patterns.\\n\\n## Two Types of Supervised Learning\\n\\n- **Classification** \u2014 Target is a category. Predicts discrete labels like spam/not spam, churn/no churn, or which product category a customer will buy.\\n- **Regression** \u2014 Target is a number. Predicts continuous values like house prices, salary, or laptop price.\\n\\n**How to tell which one you need:**\\n\\n```python\\nprint(df[\'Target\'].dtype)         # object/category = Classification\\nprint(df[\'Target\'].nunique())     # many unique values = likely Regression\\nprint(df[\'Target\'].value_counts()) # check class distribution for Classification\\n```\\n\\n---\\n\\n## Evaluation Metrics\\n\\n### Regression Metrics\\n\\n| Metric | What It Measures | Good Score |\\n|--------|-----------------|------------|\\n| **R\xb2** | Proportion of variance explained by the model | Closer to 1 |\\n| **MAE** | Average absolute difference between actual and predicted | Lower is better, less sensitive to outliers |\\n| **RMSE** | Square root of average squared differences | Lower is better, more sensitive to outliers |\\n\\n```python\\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\\nimport numpy as np\\n\\nr2   = r2_score(y_test, y_pred)\\nmae  = mean_absolute_error(y_test, y_pred)\\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\\n```\\n\\n---\\n\\n### Classification Metrics\\n\\n#### Confusion Matrix\\n\\n```\\n              Predicted\\n            Pos    Neg\\nActual  Pos  TP    FN\\n        Neg  FP    TN\\n```\\n\\n- **TP** = True Positives \u2014 correctly predicted positive\\n- **TN** = True Negatives \u2014 correctly predicted negative\\n- **FP** = False Positives \u2014 predicted positive, actually negative\\n- **FN** = False Negatives \u2014 predicted negative, actually positive\\n\\n| Metric | Formula | When to Use |\\n|--------|---------|-------------|\\n| **Accuracy** | (TP + TN) / Total | Balanced datasets where all classes matter equally |\\n| **Precision** | TP / (TP + FP) | When false positives are costly (e.g. spam detection) |\\n| **Recall** | TP / (TP + FN) | When false negatives are costly (e.g. medical diagnosis) |\\n| **F1-Score** | 2 \xd7 (Precision \xd7 Recall) / (Precision + Recall) | Imbalanced datasets, when both matter |\\n\\n```python\\nfrom sklearn.metrics import classification_report, confusion_matrix\\n\\nprint(classification_report(y_test, y_pred))  # Precision, Recall, F1 per class\\nprint(confusion_matrix(y_test, y_pred))\\n```\\n\\n#### Which metric to use in which scenario\\n\\n| Scenario | Use | Why |\\n|----------|-----|-----|\\n| Medical diagnosis | **Recall** | Missing a disease (FN) is more dangerous |\\n| Spam detection | **Precision** | Blocking a real email (FP) is the bigger problem |\\n| Fraud detection | **Recall** | Missing fraud (FN) is costly |\\n| Marketing campaigns | **Precision** | Avoid wasting spend on unlikely customers |\\n| Balanced dataset | **Accuracy** | All classes equally important |\\n| Imbalanced dataset | **F1-Score** | Balances precision and recall |\\n\\n---\\n\\n## Top Models\\n\\n### Regression\\n\\n| Model | Best For | Notes |\\n|-------|----------|-------|\\n| **Linear Regression** | Baseline, interpretable, linear relationships | Start here |\\n| **Random Forest Regressor** | Non-linear patterns, mixed data types | Reduces overfitting |\\n| **Gradient Boosting (XGBoost/LightGBM)** | High accuracy, complex patterns | Popular in competitions |\\n| **Support Vector Regression (SVR)** | High-dimensional data | Requires feature scaling |\\n| **Neural Networks** | Very large or complex datasets | Overkill for small data |\\n\\n### Classification\\n\\n| Model | Best For | Notes |\\n|-------|----------|-------|\\n| **Logistic Regression** | Baseline, binary classification | Simple and interpretable |\\n| **Random Forest Classifier** | Robust, mixed data types | Provides feature importance |\\n| **Gradient Boosting (XGBoost/LightGBM)** | High performance, imbalanced data | Feature selection built in |\\n| **Support Vector Machine (SVM)** | High-dimensional data | Requires feature scaling |\\n| **Neural Networks** | Image, text, complex patterns | Needs large datasets |\\n\\n---\\n\\n## Model Selection Guide\\n\\n| Data Characteristics | Recommended | Avoid |\\n|---------------------|-------------|-------|\\n| Small dataset (&lt;1000 rows) | Linear/Logistic Regression, SVM | Deep Neural Networks |\\n| Large dataset (&gt;100K rows) | Gradient Boosting, Neural Networks | KNN (slow at prediction time) |\\n| High dimensions, few samples | Linear models with regularization | Tree-based models |\\n| Mixed data types | Random Forest, Gradient Boosting | Linear models without preprocessing |\\n| Need interpretability | Linear Regression, Decision Trees | Neural Networks, Ensembles |\\n| Non-linear relationships | Tree-based models, Neural Networks | Simple linear models |\\n| Noisy data | Ensemble methods (Random Forest) | Models prone to overfitting |\\n\\n---\\n\\n## Quick Decision Framework\\n\\n**For Regression:**\\n1. Start with **Linear Regression** as baseline\\n2. Try **Random Forest** if non-linear patterns are suspected\\n3. Use **Gradient Boosting** for highest accuracy\\n4. Consider **Neural Networks** for very large or complex datasets\\n\\n**For Classification:**\\n1. Start with **Logistic Regression** as baseline\\n2. Try **Random Forest** for robustness\\n3. Use **Gradient Boosting** for competitive performance\\n4. Consider **SVM** for high-dimensional data\\n5. Use **Neural Networks** for image, text, or highly complex data\\n\\n---\\n\\n## Key Takeaways\\n\\n- **Always start simple** \u2014 Linear/Logistic Regression first. A strong baseline is more useful than jumping straight to complex models.\\n- **Understand your data** \u2014 Dataset size, feature types, and noise level all affect which model to reach for.\\n- **Pick metrics based on business context** \u2014 A high accuracy score can hide a useless model on imbalanced data. Know the cost of FP vs FN in your problem.\\n- **Feature engineering often beats model complexity** \u2014 Clean, relevant features matter more than which model you pick.\\n- **Cross-validate** \u2014 A single train/test split can mislead you. Use cross-validation for a reliable performance estimate."},{"id":"/2025/10/02/copilitusage","metadata":{"permalink":"/2025/10/02/copilitusage","source":"@site/blog/2025-10-02-copilitusage.md","title":"My Daily Workflow with Enterprise Copilot: A Technical Writer\'s Field Notes","description":"Field notes from using Microsoft Enterprise Copilot as a technical writer \u2014 prompting strategies, real use cases, and what actually works for drafting and reviewing docs.","date":"2025-10-02T00:00:00.000Z","tags":[{"inline":true,"label":"Gen AI","permalink":"/tags/gen-ai"},{"inline":true,"label":"AI Tools","permalink":"/tags/ai-tools"},{"inline":true,"label":"Technical Writing","permalink":"/tags/technical-writing"}],"readingTime":5.33,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"My Daily Workflow with Enterprise Copilot: A Technical Writer\'s Field Notes","description":"Field notes from using Microsoft Enterprise Copilot as a technical writer \u2014 prompting strategies, real use cases, and what actually works for drafting and reviewing docs.","tags":["Gen AI","AI Tools","Technical Writing"]},"unlisted":false,"prevItem":{"title":"Supervised Learning: A Quick Reference","permalink":"/2025/10/28/supervised-learning"},"nextItem":{"title":"Pandas GroupBy in Practice: Real-World Scenarios and What to Watch Out For","permalink":"/2025/09/13/pandas-groupby"}},"content":"A few months ago, I shared my experience with [Msty as an AI aggregator](/2025/08/08/msty) for personal AI projects. Today, I want to talk about how in work we use Enterprise Copilot in the technical writers team.\\n\\nHere\'s the thing - documentation work involves a lot of back-and-forth. You get requirements documents, user stories, and sometimes details in emails. Your job is to comprehend and abstract the data to create user guides and release notes. It takes time, multiple drafts, and constant clarifications because information is scattered and not complete.\\n\\nIn our organization, we have access to Enterprise Copilot, and due to data privacy issues, other AI chat tools are not allowed for project work.\\n\\n**From Blank Pages to First Drafts**\\n\\nBefore Copilot, creating the first draft took time. New features or technical terms would come up without context, and would need to spend hours figuring out what they meant. The gap in details or context could lead to incomplete documentation and more review cycles.\\n\\nI started experimenting with Copilot cautiously - simple prompts for language review. Generic requests like \\"improve this text\\" gave mediocre results. But when I got specific - \\"Review this paragraph for passive voice\\" or \\"Check if this matches our conversational tone\\" - the output improved.\\n\\nIt became clear that to get the expected outcome, Co-pilot required  clear instructions.\\n\\n**How I Actually Use It**\\n\\nTraining and guidelines are given to the team members on how to use Copilot. There is a set of prompt templates created for the various tasks in the team. If there are tweaks required for different products, the template is customized as required. This helps junior team members to generate drafts faster without starting from scratch. Also, it ensures that the output is consistent in tone and style.\\n\\nFor example: \\"Based on this SRS document, explain the core user workflow for [Product Name] in simple terms. List any assumptions you\'re making.\\"\\n\\nThis surfaces gaps early and gives everyone a solid starting point.\\n\\nFor style consistency, I use specific prompts: \\"Review this section for conversational tone, active voice, and second-person perspective. Highlight deviations.\\" It catches inconsistencies before documents reach reviewers.\\n\\n**Finding What\'s Missing**\\n\\nOne trick I love - using different personas to test the documents. I ask Copilot: \\"Act as a developer implementing this API. What information is missing? What questions would you have?\\"\\n\\nOr: \\"Act as a business analyst explaining this feature to stakeholders. What\'s unclear?\\"\\n\\nThis helps me spot gaps and create a list of questions for SMEs and product owners before lengthy review cycles start.\\n\\n**Working with Multiple Documents**\\n\\nEnterprise Copilot\'s ability to work with OneDrive links is useful. Instead of opening five different documents to cross-reference information, I upload links and query: \\"What are the differences in how authentication is described across these five documents?\\"\\n\\nThis surfaces inconsistencies and helps me create accurate abstractions of complex requirements.\\n\\n**Creating Templates Faster**\\n\\nRecently, when we needed a new document template, I asked Copilot to draft an outline based on the template\'s purpose, our existing formats, and standards. The initial draft wasn\'t perfect, but it accelerated iterations with the product team. What would have taken multiple meetings, happened in 1-2 focused sessions.\\n\\n**Building Glossaries**\\n\\nIdentifying terms for glossaries manually is a time-consuming activity. Now I upload documentation and ask: \\"Identify technical terms and acronyms that would need definitions for a non-technical audience.\\"\\n\\nIt\'s not always perfect - sometimes it flags common terms or misses domain-specific terms - but it gives a solid starting list to refine.\\n\\n**The Reality Check**\\n\\nLet me be clear: Enterprise Copilot isn\'t a silver bullet.\\n\\nGeneric prompts don\'t work. Different products have different input documents and formats. A prompt that works beautifully for one product may produce less desirable output for another. I\'ve had to fine-tune prompts for each scenario and teach my junior team members how to customize them for specific tasks.\\n\\nGuided adoption is critical. Initially, I told the team, \\"We have Copilot now, explore and use it.\\" Adoption was slow. When I started showing them exactly how to use it for their daily tasks - \\"Here\'s how you generate a first draft of user guide from user stories\\" - adoption improved dramatically. When guided for their daily tasks, people actually use it.\\n\\nAlways keep a human in the loop. I treat Copilot as a partner or thought assistant, not a replacement for critical thinking. Every output needs review. Every draft needs validation against source documents. AI accelerates the process; it doesn\'t make you less responsible.\\n\\nKnow when NOT to use it. For repetitive tasks that can be automated with Excel functions or scripts, I don\'t ask Copilot to generate the final output. Instead, I ask it how to use the right functions or create the script. For example: \\"What Excel function extracts dates from text in this format?\\" This teaches me the underlying skill rather than making me dependent on AI for every small task.\\n\\n**The Real Impact**\\n\\nUsing Enterprise Copilot has genuinely accelerated my documentation lifecycle. Drafts happen faster. Review cycles are shorter. Cross-functional collaboration is smoother. The team feels more confident experimenting with documentation early in the process.\\n\\nBut the tool is only as good as how you use it. Fine-tune your prompts. Understand your workflows. Keep learning. And always keep the human in the loop.\\n\\nThe core principle I started with remains true: GenAI accelerates content, streamlines processes, and manages knowledge efficiently. But how you implement it - the prompts you write, the workflows you design, the adoption strategies you use - that\'s where the real work happens.\\n\\nAnd that work? It\'s worth it.\\n\\n**Who Should Try This?**\\n\\nIf you are a knowledge worker and your organization has Enterprise Copilot (or similar tools), experiment with it hands-on. Start small with language review, then expand to drafting, then to gap analysis.\\n\\nBetter yet, show your junior team members exactly how to use it for their specific tasks. Don\'t just say \\"use AI\\" - demonstrate the actual prompts and workflows that work.\\n\\nSometimes the best tools are the ones that stay out of your way and let you focus on getting things done. Enterprise Copilot does that for me - not by replacing my work, but by accelerating the parts that used to slow everything down.\\n\\n---\\n\\n*What\'s your experience with AI tools in documentation work? I\'d love to hear what\'s working (or not working) for you.*"},{"id":"/2025/09/13/pandas-groupby","metadata":{"permalink":"/2025/09/13/pandas-groupby","source":"@site/blog/2025-09-13-pandas-groupby.md","title":"Pandas GroupBy in Practice: Real-World Scenarios and What to Watch Out For","description":"Practical guide to GroupBy and aggregation using business scenarios \u2014 sales performance, campaign ROI, and support ticket analysis. Covers the gotchas learners commonly hit.","date":"2025-09-13T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"data analysis","permalink":"/tags/data-analysis"},{"inline":true,"label":"learning","permalink":"/tags/learning"}],"readingTime":5.03,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Pandas GroupBy in Practice: Real-World Scenarios and What to Watch Out For","description":"Practical guide to GroupBy and aggregation using business scenarios \u2014 sales performance, campaign ROI, and support ticket analysis. Covers the gotchas learners commonly hit.","tags":["python","data analysis","learning"]},"unlisted":false,"prevItem":{"title":"My Daily Workflow with Enterprise Copilot: A Technical Writer\'s Field Notes","permalink":"/2025/10/02/copilitusage"},"nextItem":{"title":"Numpy: A Quick Reference","permalink":"/2025/08/15/numpy"}},"content":"When you work with real datasets, raw rows rarely tell the story \u2014 you need to roll them up. GroupBy is how you do that.\\n\\nIn these scenarios the data comes in as individual transactions or records, but the business question is always at a higher level: *How did each Sales Manager\'s team perform? Which Marketing Manager ran the most efficient campaigns? Which Team Lead resolves tickets the fastest?*\\n\\nThis post walks through three of those scenarios and highlights the things that trip people up along the way.\\n\\n---\\n\\n## Scenario 1: Sales Performance by Manager\\n\\n**Dataset:** Individual sales transactions \u2014 each row is one sale, with columns for Salesperson, Sales Manager, Sale Amount, Region, Product Category, and Sale Date.\\n\\n**Goal:** Produce one summary row per Sales Manager showing total revenue, number of transactions, team size, and average deal size.\\n\\n### Grouping by a single column\\n\\n```python\\ngrouped = df.groupby(\'Sales_Manager\')[\'Sale_Amount\'].sum()\\n```\\n\\nThis gives you total revenue per manager \u2014 but it returns a **Series** with Sales_Manager as the index, not a column.\\n\\n### Grouping by multiple columns\\n\\nWhen you want a breakdown by Manager **and** Region:\\n\\n```python\\ngrouped = df.groupby([\'Sales_Manager\', \'Region\'])[\'Sale_Amount\'].sum()\\n```\\n\\nPass a list of column names. The result now has a **MultiIndex** (two-level index). To flatten it back into a regular DataFrame with normal columns:\\n\\n```python\\ngrouped = grouped.reset_index()\\n```\\n\\n### Aggregating multiple metrics at once\\n\\nInstead of calling `.groupby()` separately for each metric, use `.agg()` with a dictionary \u2014 one call, one clean result:\\n\\n```python\\nsummary = df.groupby(\'Sales_Manager\').agg(\\n    Total_Revenue=(\'Sale_Amount\', \'sum\'),\\n    Transaction_Count=(\'Transaction_ID\', \'count\'),\\n    Avg_Deal_Size=(\'Sale_Amount\', \'mean\'),\\n    Team_Size=(\'Salesperson\', \'nunique\')\\n).reset_index()\\n```\\n\\n**Watch out \u2014 `count()` vs `nunique()`:**\\n- `count()` counts all non-null rows in the group\\n- `nunique()` counts how many *distinct* values exist\\n\\nIf you use `count()` to measure Team Size, you\'ll get the number of transactions, not the number of unique salespeople. Use `nunique()` when the question is \\"how many different X\\".\\n\\n### `as_index=False` \u2014 when do you need it?\\n\\nBy default, the column you group by becomes the index of the result:\\n\\n```python\\ndf.groupby(\'Sales_Manager\')[\'Sale_Amount\'].sum()\\n# Sales_Manager is now the index, not a regular column\\n```\\n\\nUse `as_index=False` to keep it as a regular column \u2014 especially useful when you want to merge the result back into another DataFrame:\\n\\n```python\\ndf.groupby(\'Sales_Manager\', as_index=False)[\'Sale_Amount\'].sum()\\n```\\n\\nAlternatively, chain `.reset_index()` after `.agg()` \u2014 same effect. Pick one and be consistent. With `.agg()`, the habit of ending with `.reset_index()` is cleaner since you\'re already building multiple columns at once.\\n\\n---\\n\\n## Scenario 2: Campaign ROI \u2014 Deriving Metrics After Aggregation\\n\\n**Dataset:** Individual campaign executions \u2014 each row is one campaign run with columns for Marketing Manager, Budget, Revenue, Impressions, Clicks, and Conversions.\\n\\n**Goal:** One row per Marketing Manager showing ROI %, Click-Through Rate, Conversion Rate, and Cost Per Conversion.\\n\\n### Aggregate first, then calculate\\n\\nThe key insight: **you can\'t calculate ROI inside `.agg()`**. `.agg()` operates column by column. ROI needs both Revenue and Budget \u2014 two columns. So the pattern is: aggregate to get the totals, then derive the metrics on the resulting DataFrame.\\n\\n```python\\nsummary = df.groupby(\'Marketing_Manager\').agg(\\n    Total_Budget=(\'Budget\', \'sum\'),\\n    Total_Revenue=(\'Revenue\', \'sum\'),\\n    Total_Clicks=(\'Clicks\', \'sum\'),\\n    Total_Impressions=(\'Impressions\', \'sum\'),\\n    Total_Conversions=(\'Conversions\', \'sum\'),\\n    Campaigns_Run=(\'Campaign_ID\', \'count\')\\n).reset_index()\\n```\\n\\nNow derive the metrics on `summary`:\\n\\n```python\\nsummary[\'ROI_%\'] = ((summary[\'Total_Revenue\'] - summary[\'Total_Budget\'])\\n                    / summary[\'Total_Budget\'] * 100).round(2)\\n\\nsummary[\'CTR_%\'] = (summary[\'Total_Clicks\']\\n                    / summary[\'Total_Impressions\'] * 100).round(2)\\n\\nsummary[\'Conversion_Rate_%\'] = (summary[\'Total_Conversions\']\\n                                 / summary[\'Total_Clicks\'] * 100).round(2)\\n\\nsummary[\'Cost_Per_Conversion\'] = (summary[\'Total_Budget\']\\n                                   / summary[\'Total_Conversions\']).round(2)\\n```\\n\\nThis keeps the logic clean \u2014 aggregation in one block, derived metrics in a second block.\\n\\n**Tip:** If you want formatted output (e.g., currency or percentage strings), do that as a final display step, not on the working columns. Keep the numeric columns numeric so you can still sort or filter on them.\\n\\n```python\\n# For display only\\nsummary[\'ROI_%\'].map(\'{:.2f}%\'.format)\\n```\\n\\n---\\n\\n## Scenario 3: Support Ticket Analysis \u2014 Filtering Groups and Conditional Counts\\n\\n**Dataset:** Support tickets \u2014 each row is one ticket with columns for Agent, Team Lead, Resolution Time (hours), Priority (Low/Medium/High/Critical), Customer Satisfaction Score (1\u20135), and Department.\\n\\n**Goal:** Team-level summary with average resolution time, satisfaction rate, and count of critical tickets handled.\\n\\n### Counting conditional rows within a group\\n\\nHow many Critical priority tickets did each Team Lead\'s team handle? You can\'t use a plain `count()` for this \u2014 you need to count only rows where Priority == \'Critical\'.\\n\\nOne approach: filter first, then group:\\n\\n```python\\ncritical_counts = df[df[\'Priority\'] == \'Critical\']\\\\\\n    .groupby(\'Team_Lead\')[\'Ticket_ID\'].count()\\\\\\n    .reset_index()\\\\\\n    .rename(columns={\'Ticket_ID\': \'Critical_Tickets\'})\\n```\\n\\nA cleaner approach: create a flag column, then aggregate it:\\n\\n```python\\ndf[\'Is_Critical\'] = (df[\'Priority\'] == \'Critical\').astype(int)\\n\\nsummary = df.groupby(\'Team_Lead\').agg(\\n    Total_Tickets=(\'Ticket_ID\', \'count\'),\\n    Avg_Resolution_Hours=(\'Resolution_Time\', \'mean\'),\\n    Avg_Satisfaction=(\'Satisfaction_Score\', \'mean\'),\\n    Critical_Tickets=(\'Is_Critical\', \'sum\')\\n).reset_index()\\n```\\n\\nThe flag column (0 or 1) means `sum()` gives you the count of matching rows \u2014 a useful pattern for any conditional count inside `.agg()`.\\n\\n### Satisfaction Rate \u2014 percentage of tickets rated 4 or above\\n\\nSame pattern \u2014 create a flag, then sum it:\\n\\n```python\\ndf[\'High_Satisfaction\'] = (df[\'Satisfaction_Score\'] >= 4).astype(int)\\n\\nsummary = df.groupby(\'Team_Lead\').agg(\\n    Total_Tickets=(\'Ticket_ID\', \'count\'),\\n    High_Sat_Count=(\'High_Satisfaction\', \'sum\')\\n).reset_index()\\n\\nsummary[\'Satisfaction_Rate_%\'] = (summary[\'High_Sat_Count\']\\n                                   / summary[\'Total_Tickets\'] * 100).round(1)\\n```\\n\\n### Using `.filter()` on groups\\n\\n`.filter()` is different from `.agg()` \u2014 it doesn\'t collapse groups. It returns the **original rows** from groups that meet a condition. Use it when you want to subset the data to only certain groups:\\n\\n```python\\n# Keep only rows belonging to teams with avg satisfaction >= 4.0\\nhigh_performing_teams = df.groupby(\'Team_Lead\').filter(\\n    lambda x: x[\'Satisfaction_Score\'].mean() >= 4.0\\n)\\n```\\n\\nThis returns a DataFrame with all the original columns \u2014 not a summary. Useful when you want to drill into high or low performing groups for further analysis.\\n\\n---\\n\\n## Common Gotchas Summary\\n\\n| Situation | What to do |\\n|-----------|------------|\\n| Group column becomes the index | Chain `.reset_index()` or use `as_index=False` |\\n| Need distinct count per group | Use `nunique()`, not `count()` |\\n| Need a metric that uses two columns (ROI, rate) | Calculate it after `.agg()`, not inside it |\\n| Grouped by two columns and got MultiIndex | `.reset_index()` flattens it |\\n| Need to count rows matching a condition per group | Create a 0/1 flag column, then `sum()` it in `.agg()` |\\n| Want to keep original rows for only some groups | Use `.filter()`, not `.agg()` |"},{"id":"/2025/08/15/numpy","metadata":{"permalink":"/2025/08/15/numpy","source":"@site/blog/2025-08-15-numpy.md","title":"Numpy: A Quick Reference","description":"A quick reference guide to NumPy, the foundational Python library for numerical computing, covering arrays, broadcasting, vectorization, and mathematical functions.","date":"2025-08-15T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"data analysis","permalink":"/tags/data-analysis"},{"inline":true,"label":"personal project","permalink":"/tags/personal-project"}],"readingTime":2.11,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Numpy: A Quick Reference","description":"A quick reference guide to NumPy, the foundational Python library for numerical computing, covering arrays, broadcasting, vectorization, and mathematical functions.","tags":["python","data analysis","personal project"]},"unlisted":false,"prevItem":{"title":"Pandas GroupBy in Practice: Real-World Scenarios and What to Watch Out For","permalink":"/2025/09/13/pandas-groupby"},"nextItem":{"title":"Maximizing AI Value: Comparing Models and Saving Money with Msty","permalink":"/2025/08/08/msty"}},"content":"NumPy (Numerical Python) is a powerful open-source library for numerical and scientific computing in Python. It provides support for large, multi-dimensional arrays and a wide variety of mathematical functions to operate on them efficiently.\\n\\nNumPy is widely used in **data science, machine learning, artificial intelligence, image processing, and scientific research**. It serves as the foundation for many higher-level libraries like Pandas, SciPy, Scikit-learn, and TensorFlow.\\n\\n---\\n\\n## Key Features\\n- **N-Dimensional Arrays**: Fast and memory-efficient data structures for numerical data.\\n- **Broadcasting**: Perform elementwise operations on arrays of different shapes without explicit loops.\\n- **Vectorization**: Replace slow Python loops with efficient array operations.\\n- **Mathematical Functions**: Built-in linear algebra, Fourier transforms, and random number generation.\\n- **Integration with C/C++/Fortran**: Enables high-performance computations.\\n\\n---\\n\\n## Quick Reference\\n\\n#### 1. Array Creation\\n*Create arrays from lists, ranges, and random values.*\\n```python\\nimport numpy as np\\n\\nnp.array([1, 2, 3])          # From list\\nnp.arange(0, 10, 2)          # Range with step\\nnp.linspace(0, 1, 5)         # 5 values between 0 and 1\\nnp.zeros((2,3))              # 2x3 array of zeros\\nnp.ones((3,3))               # 3x3 array of ones\\nnp.random.rand(2,2)          # 2x2 random values [0,1)\\n```\\n\\n#### 2. Array Properties & Reshaping\\n*Inspect properties and change shape of arrays.*\\n```python\\narr = np.arange(12).reshape(3,4)\\narr.shape      # (3,4)\\narr.ndim       # 2\\narr.size       # 12\\narr.dtype      # int32/64\\narr.reshape(2,6)\\narr.flatten()  # Returns copy\\narr.ravel()    # Returns view\\n```\\n\\n#### 3. Indexing & Slicing\\n*Access and filter array elements.*\\n```python\\nA = np.arange(1,26).reshape(5,5)\\nA[1,:]             # 2nd row\\nA[:, -1]           # Last column\\nA[1:4,1:4]         # 3x3 block\\nA[::-1, ::-1]      # Reverse rows & cols\\nA[A % 2 == 0]      # Boolean mask\\n```\\n\\n#### 4. Arithmetic & Aggregations\\n*Elementwise operations and reductions.*\\n```python\\nx = np.array([1,2,3])\\ny = np.array([4,5,6])\\nx + y             # [5 7 9]\\nnp.exp(x)         # Exponential\\nnp.mean(y)        # Mean\\nnp.sum(y)         # Sum\\nnp.argmax(y)      # Index of max\\n```\\n\\n#### 5. Broadcasting\\n*Automatic expansion of dimensions to match shapes.*\\n```python\\nA = np.array([[1],[2],[3]])\\nB = np.array([10,20,30])\\nA + B   # Shape (3,3)\\n```\\n\\n#### 6. Linear Algebra\\n*Matrix operations and decompositions.*\\n```python\\nM = np.array([[1,2],[3,4]])\\nnp.dot(M, M)            # Matrix multiplication\\nnp.linalg.det(M)        # Determinant\\nnp.linalg.inv(M)        # Inverse\\nnp.linalg.eig(M)        # Eigenvalues & eigenvectors\\n```\\n\\n#### 7. Random Numbers\\n*Generate random values and reproducibility.*\\n```python\\nnp.random.seed(42)\\nnp.random.randint(0,10,5)   # 5 random integers [0,10)\\nnp.random.rand(3,3)         # Uniform [0,1)\\nnp.random.randn(2,2)        # Normal(0,1)\\n```\\n\\n#### 8. Utilities\\n*Handy functions for data manipulation.*\\n```python\\narr = np.array([3,1,2,3,2,1,1,4])\\nnp.unique(arr)                 # Unique values\\nnp.sort(arr)                   # Sorted array\\nnp.argsort(arr)                # Indices for sorting\\nnp.concatenate(([1,2],[3,4]))  # [1 2 3 4]\\nnp.split(np.arange(10), 2)     # Split into 2 parts\\nnp.tile([1,2], 3)              # Repeat sequence\\nnp.repeat([1,2], 3)            # Repeat elements\\nnp.clip([-1, 2, 5], 0, 3)      # Limit values\\n```\\n\\n---"},{"id":"/2025/08/08/msty","metadata":{"permalink":"/2025/08/08/msty","source":"@site/blog/2025-08-08-msty.md","title":"Maximizing AI Value: Comparing Models and Saving Money with Msty","description":"How Msty lets you connect multiple AI APIs (Claude, GPT-4, Gemini) under one roof and pay only for what you use, without monthly subscription lock-in.","date":"2025-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"Gen AI","permalink":"/tags/gen-ai"},{"inline":true,"label":"AI tools","permalink":"/tags/ai-tools"},{"inline":true,"label":"personal project","permalink":"/tags/personal-project"}],"readingTime":2.49,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Maximizing AI Value: Comparing Models and Saving Money with Msty","description":"How Msty lets you connect multiple AI APIs (Claude, GPT-4, Gemini) under one roof and pay only for what you use, without monthly subscription lock-in.","tags":["Gen AI","AI tools","personal project"]},"unlisted":false,"prevItem":{"title":"Numpy: A Quick Reference","permalink":"/2025/08/15/numpy"},"nextItem":{"title":"Pandas: A Quick Reference","permalink":"/2025/08/01/pandas"}},"content":"I recently stumbled across [Msty](https://msty.app/), and it solved a problem I\'d been having for some time.\\n\\nHere\'s the thing - I wanted to try ChatGPT Plus or Claude Pro, but I\'m not a heavy AI user. Paying $20 every month when I might use it a few times felt like throwing money away.\\n\\nSome days you\'re in the mood to experiment, but the free tier isn\'t enough. Msty solved this perfectly - you just plug in your API key and pay for what you use. No monthly guilt, no \\"I better use this 20 times to get my money\'s worth\\" pressure.\\n\\n**Switching Between Different AI Models**\\n\\nWhat\'s cool is you can connect multiple APIs - Claude, OpenAI, Gemini, you name it. I can ask the same question to different models and compare their responses. Ever wondered, \\"Would GPT-4 answer this differently than Claude?\\" Now you can find out without juggling different apps.\\n\\nThe whole setup works seamlessly without making you mess with API endpoints or technical configurations.\\n\\nThey also let you run smaller models locally on your machine for basic tasks. You get a list of featured local models with compatibility scores for your laptop. One glance tells you which model suits your needs and hardware best.\\n\\nIt\'s like having a Swiss Army knife for AI.\\n\\n**Knowledge Stack - Your Personal AI Library**\\n\\nThis feature genuinely impressed me. You can upload your documents and use either local or remote AI models for embeddings. The settings are customizable, but the defaults work fine for most cases.\\n\\nI tested this for my kid\'s exam prep. After uploading previous years\' question papers, I asked Claude (through Msty) to generate similar practice questions. The results were spot-on. The AI understood the format, difficulty level, and topics - creating new questions that matched the requirements.\\n\\nSetting up took minutes. Paste your API key, drag and drop your files, and done. They even recommend which local models will run smoothly on your specific hardware.\\n\\n\\n**The Real Cost Savings**\\n\\nInstead of paying for subscriptions I barely use, my actual API usage costs under $1 per month. For someone who needs AI occasionally but wants the best models when I do, this approach makes perfect sense.\\n\\nRunning local models for simple tasks cuts costs even further.\\n\\n**Who Should Try This?**\\n\\nIf you\'re a beginner wanting access to top-tier models without expensive monthly commitments, Msty is worth exploring. It\'s handy for personal projects where your AI needs fluctuate. Some weeks, I don\'t do any AI learning; other weeks, I\'m experimenting with AI. Now I only pay for what I use.\\n\\nI found Msty a powerful ally. It\'s not trying to be everything to everyone - just a straightforward way to access AI models without subscription overhead.\\n\\nGive it a try if you\'re on the fence about AI subscriptions but still want to experiment with the good stuff. Sometimes the best tools are the ones that stay out of your way and let you focus on getting things done."},{"id":"/2025/08/01/pandas","metadata":{"permalink":"/2025/08/01/pandas","source":"@site/blog/2025-08-01-pandas.md","title":"Pandas: A Quick Reference","description":"A concise quick reference for the Pandas Python library, covering Series, DataFrames, key features, and common data manipulation operations.","date":"2025-08-01T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"data analysis","permalink":"/tags/data-analysis"},{"inline":true,"label":"learning","permalink":"/tags/learning"}],"readingTime":2.63,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Pandas: A Quick Reference","description":"A concise quick reference for the Pandas Python library, covering Series, DataFrames, key features, and common data manipulation operations.","tags":["python","data analysis","learning"]},"unlisted":false,"prevItem":{"title":"Maximizing AI Value: Comparing Models and Saving Money with Msty","permalink":"/2025/08/08/msty"},"nextItem":{"title":"Enterprise Content Evolution: Migrating Platforms","permalink":"/2025/07/28/project1"}},"content":"- Open source library for Data Analysis and Data manipulation\\n- Built on top of NumPy and integrates well with other libraries like Matplotlib, Seaborn, Scikit-learn\\n\\n2 primary data structures\\n- Series - 1 D array\\n- DataFrames - 2 D array heterogeneous table like Excel, SQL, with rows and columns.\\n\\n## Key Features\\n- **Easy Handling of Missing Data** - - With functions like .fillna(), .dropna(), and more.\\n\\n- **Data Alignment and Indexing** allows for label-based and position-based data selection and slicing.\\n \\n- **Data Filtering and Transformation** - using conditions, and transformation using apply(), map(), etc.\\n\\n- **Merging and Joining** - Similar to SQL joins via merge(), join(), and concat() functions.\\n\\n- **GroupBy Functionality** - Split data into groups, apply operations, and combine results.\\n\\n- **Time Series Support** - Built-in support for date and time operations.\\n\\n- **Read/Write from Multiple File Formats** - Supports CSV, Excel, JSON, SQL, and more.\\n\\n\\n## Quick Reference\\n\\n### Load a CSV File\\n\\n```python\\ndf = pd.read_csv(\'data.csv\')\\n```\\n\\n### Dropping a Column\\n\\n```python\\ndf = df.drop(\'column_name\', axis=1)\\n```\\n\\n###  Filling Missing Values \\n\\n- with Median\\n\\n```python\\ndf[\'col\'] = df[\'col\'].fillna(df[\'col\'].median())\\n```\\n\\n- with Mode\\n\\n```python\\ndf[\'col\'] = df[\'col\'].fillna(df[\'col\'].mode()[0])\\n```\\n\\n### Combining Two Columns into One\\n\\n```python\\ndf[\'combined\'] = df[\'col1\'].astype(str) + \'_\' + df[\'col2\'].astype(str)\\n```\\n\\n### Converting to Numeric (with errors turned to NaN)\\n\\n```python\\ndf[\'col\'] = pd.to_numeric(df[\'col\'], errors=\'coerce\')\\n```\\n\\n### Filtering Rows Based on Condition\\n\\n```python\\nfiltered_df = df[df[\'col\'] > 10]\\n```\\n### One-Hot Encoding Categorical Features\\n\\n```python\\ndf = pd.get_dummies(df, columns=[\'category_column\'])\\n```\\n\\n### Label Encoding\\n\\n```python\\ndf[\'encoded\'] = df[\'category_column\'].astype(\'category\').cat.codes\\n```\\n\\n### Rename Columns\\n\\n```python\\ndf = df.rename(columns={\'old_name\': \'new_name\'})\\n```\\n\\n### Groupby and Aggregate\\n\\n```python\\ngrouped = df.groupby(\'category\')[\'col\'].mean()\\n```\\n\\n### Sorting DataFrame\\n\\n```python\\ndf = df.sort_values(by=\'col\', ascending=False)\\n```\\n\\n### Reset Index\\n\\n```python\\ndf = df.reset_index(drop=True)\\n```\\n\\n### Selecting Multiple Columns\\n\\n```python\\nselected = df[[\'col1\', \'col2\', \'col3\']]\\n```\\n\\n### Date time operations\\n-  Conversion to date time column\\n```python\\ndf[\'date_column\'] = pd.to_datetime(df[\'date_column\'])\\n```\\n\\n-  Extract Date, Year, Month, and Day\\n```python\\ndf[\'date\'] = df[\'date_column\'].dt.date        # Extract date\\ndf[\'year\'] = df[\'date_column\'].dt.year        # Extract year\\ndf[\'month\'] = df[\'date_column\'].dt.month      # Extract month\\ndf[\'day\'] = df[\'date_column\'].dt.day          # Extract day\\n```\\n\\n- Extract Time Component\\n```python\\ndf[\'time\'] = df[\'date_column\'].dt.time\\n```\\n\\n- Combine Year, Month, Day, and Time Columns into a Single Datetime\\n```python\\ndf[\'datetime\'] = pd.to_datetime(df[[\'year\', \'month\', \'day\', \'hour\', \'minute\', \'second\']])\\n```\\n\\n- Filter or Select Data for a Specific Time Period\\n```python\\ndf[(df[\'date_column\'] >= \'2023-01-01\') & (df[\'date_column\'] < \'2024-01-01\')]\\n```\\n\\nTip: Use .dt accessor for various datetime features (like dt.hour, dt.minute, dt.dayofweek, etc.) to further engineer time-related columns or extract more granularity from your datetime data\\n\\n### Replace Operations\\n\\n- Replace specific characters\\n\\n*Replace spaces and \'/\' with NaN in all columns*\\n```python\\ndf.replace([\' \', \'/\'], np.nan, inplace=True)\\n```\\n\\n*Replace only in a specific column*\\n```python\\ndf[\'col\'] = df[\'col\'].replace([\' \', \'/\'], np.nan)\\n```\\n\\n- Replace using Regex\\n\\n*Replace spaces and \'/\' characters with NaN using regex in a column*\\n```python\\ndf[\'col\'] = df[\'col\'].replace(r\'[ /]\', np.nan, regex=True)\\n```\\n*To replace with another value (e.g., \'missing\')*\\n```python\\ndf[\'col\'] = df[\'col\'].replace(r\'[ /]\', \'missing\', regex=True)\\n```\\n\\nTip:\\n\\n- Use np.nan to replace with NaN.\\n- Use the regex=True argument to match patterns (e.g., all whitespace or special characters).\\n- Set inplace=True to modify the DataFrame directly.\\n- These patterns work for spaces, slashes, or any regex you specify!"},{"id":"/2025/07/28/project1","metadata":{"permalink":"/2025/07/28/project1","source":"@site/blog/2025-07-28-project1.md","title":"Enterprise Content Evolution: Migrating Platforms","description":"How I led the migration of an enterprise knowledge portal from SharePoint to Sitecore, redesigning the information architecture for a microservices-era product suite.","date":"2025-07-28T00:00:00.000Z","tags":[{"inline":true,"label":"Sitecore","permalink":"/tags/sitecore"},{"inline":true,"label":"SharePoint","permalink":"/tags/share-point"},{"inline":true,"label":"Enterprise Platform Migration","permalink":"/tags/enterprise-platform-migration"}],"readingTime":1.7,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Enterprise Content Evolution: Migrating Platforms","description":"How I led the migration of an enterprise knowledge portal from SharePoint to Sitecore, redesigning the information architecture for a microservices-era product suite.","tags":["Sitecore","SharePoint","Enterprise Platform Migration"]},"unlisted":false,"prevItem":{"title":"Pandas: A Quick Reference","permalink":"/2025/08/01/pandas"}},"content":"**Driving content systems evolution and GenAI adoption for a multi-product digital banking suite**\\n\\n### \ud83d\udd27 **From SharePoint to Sitecore: A Strategic Platform Shift**\\n\\nIn 2022, I led the full migration of our enterprise knowledge portal \u2014 not because it failed, but because it outgrew its purpose.\\n\\n- **Why we moved:**\\n    \\n    SharePoint 2013 was reaching end-of-life, and our product suite had evolved from monolithic apps to a platform-based, microservices architecture. The old Information Architecture couldn\u2019t scale with this growth, and our SharePoint portal dev team had migrated to open-source stacks. We needed a **future-ready, flexible platform** \u2014 fast.\\n    \\n- **What I did:**\\n    \\n    My manager and I evaluated different platform options, presented a Sitecore business case to senior leadership, and got approval. Then came the heavy lift:\\n    \\n    - **Redesigned the IA** to reflect the new product structure while mapping 1TB of legacy content into the new model.\\n    - **Remodeled metadata strategy** to ensure search relevance and scalability.\\n    - **Coordinated across multiple teams** \u2014 product, engineering, security, data privacy, infra \u2014 to align with enterprise standards.\\n    - **Partnered with the implementation team** to test each feature under tight delivery timelines and provide ongoing feedback.\\n    - **Drove launch readiness** \u2014 created onboarding guides, planned internal/external comms, and incorporated user feedback into post-launch refinements.\\n\\n> Result: A modern, secure, scalable publishing platform that supports evolving product architecture and accelerates future AI integration.\\n> \\n\\n### \ud83d\udcac **GenAI Enablement at Scale**\\n\\nWhile I didn\u2019t build KnowledgeGPT myself, I played a major role in its adoption and rollout:\\n\\n- Collaborated with the AI team to bring **ChatGPT-style search** into the knowledge platform.\\n- Led prompt strategy and internal use-case validation.\\n- Rolled out tools like a **language review assistant** and a **prompt repository**, helping our writers extract, comprehend, and standardize content, reducing review overhead by 70%.\\n\\n### \ud83d\udd01 **End-to-End Platform Ownership**\\n\\n- Manage the **publishing portal roadmap**: features, releases, vendor collaboration, and continuous UX improvements.\\n- Onboard and support partners, internal teams, and customers across content access and usage.\\n- Leading a team of 5, handling the knowledge platform and content delivery of Digital Banking products, while coordinating across multiple teams."}]}}')}}]);