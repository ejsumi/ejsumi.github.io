"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[612],{1662(e,n,r){r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var s=r(805),t=r(4848),i=r(8453);const o={title:"Machine Learning Practice Problems: Classification vs Regression",description:"A practical walkthrough of ML problems with both categorical and numerical targets \u2014 what changes in your pipeline, how to encode correctly, which metrics to use, and the gotchas to watch out for.",tags:["python","machine learning","learning"]},a=void 0,c={authorsImageUrls:[]},l=[{value:"First: How to Tell What You&#39;re Working With",id:"first-how-to-tell-what-youre-working-with",level:2},{value:"What Changes Between the Two",id:"what-changes-between-the-two",level:2},{value:"Regression Scenarios",id:"regression-scenarios",level:2},{value:"Laptop Price Prediction \u2014 Random Forest Regressor",id:"laptop-price-prediction--random-forest-regressor",level:3},{value:"Used Car Prices \u2014 Comparing Multiple Models",id:"used-car-prices--comparing-multiple-models",level:3},{value:"Classification Scenarios",id:"classification-scenarios",level:2},{value:"Customer Churn \u2014 Binary Classification",id:"customer-churn--binary-classification",level:3},{value:"Student Course Selection \u2014 Multi-Class Classification",id:"student-course-selection--multi-class-classification",level:3},{value:"Key Gotchas Summary",id:"key-gotchas-summary",level:2}];function d(e){const n={code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"When you start working through ML practice problems, you'll notice that two problems can look almost identical \u2014 same preprocessing steps, same train/test split structure \u2014 but one has a price as the target and the other has a category. That difference changes several things in your pipeline."}),"\n",(0,t.jsx)(n.p,{children:"This post walks through real scenarios from both sides and highlights what to watch out for at each step."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"first-how-to-tell-what-youre-working-with",children:"First: How to Tell What You're Working With"}),"\n",(0,t.jsx)(n.p,{children:"Before writing a single line of model code, check your target column:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"print(df['Target'].dtype)          # object/category \u2192 Classification\n                                    # int/float \u2192 likely Regression\nprint(df['Target'].nunique())       # few unique values \u2192 Classification\nprint(df['Target'].value_counts())  # see the class distribution\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Regression targets"})," in the practice problems: ",(0,t.jsx)(n.code,{children:"Price"})," (laptops, used cars) \u2014 a continuous number."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Classification targets"})," in the practice problems: ",(0,t.jsx)(n.code,{children:"Churn"})," (Yes/No), ",(0,t.jsx)(n.code,{children:"elective"})," (DataScience/WebDev/AI...), ",(0,t.jsx)(n.code,{children:"Purchase_Category"}),", ",(0,t.jsx)(n.code,{children:"Location"}),", ",(0,t.jsx)(n.code,{children:"Flavor"}),", ",(0,t.jsx)(n.code,{children:"Main_Course"})," \u2014 all category labels."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"what-changes-between-the-two",children:"What Changes Between the Two"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Step"}),(0,t.jsx)(n.th,{children:"Regression"}),(0,t.jsx)(n.th,{children:"Classification"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Target encoding"}),(0,t.jsx)(n.td,{children:"Nothing \u2014 it's already numeric"}),(0,t.jsx)(n.td,{children:"Encode labels to numbers"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Model imports"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"RandomForestRegressor"}),", ",(0,t.jsx)(n.code,{children:"SVR"}),", etc."]}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"RandomForestClassifier"}),", ",(0,t.jsx)(n.code,{children:"LogisticRegression"}),", etc."]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Evaluation metrics"}),(0,t.jsx)(n.td,{children:"R\xb2, MAE, RMSE"}),(0,t.jsx)(n.td,{children:"Accuracy, Precision, Recall, F1"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Predicting"}),(0,t.jsx)(n.td,{children:"Returns a number"}),(0,t.jsx)(n.td,{children:"Returns a class label"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Decoding predictions"}),(0,t.jsx)(n.td,{children:"Not needed"}),(0,t.jsxs)(n.td,{children:["May need ",(0,t.jsx)(n.code,{children:"inverse_transform()"})," to get label back"]})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"Feature preprocessing (missing values, encoding, scaling) is the same regardless. The differences kick in from the model choice onwards."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"regression-scenarios",children:"Regression Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"laptop-price-prediction--random-forest-regressor",children:"Laptop Price Prediction \u2014 Random Forest Regressor"}),"\n",(0,t.jsxs)(n.p,{children:["Target: ",(0,t.jsx)(n.code,{children:"Price"})," (a number \u2014 predict what a laptop costs)"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Outlier handling before scaling:"})}),"\n",(0,t.jsx)(n.p,{children:"A step you won't always see but matters here \u2014 the laptop dataset uses IQR to filter out outlier RAM values before training:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"Q1 = df['RAM_GB'].quantile(0.25)\nQ3 = df['RAM_GB'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\n\ndf = df[(df['RAM_GB'] >= lower) & (df['RAM_GB'] <= upper)]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Outliers can skew a regressor's predictions significantly \u2014 a laptop with 1TB RAM as a data entry error will pull predictions up. Filter first, then scale."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Reserving rows for prediction:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pred = df.tail(5).copy()      # held-out rows for final prediction\ndata = df.iloc[:-5].copy()    # everything else for train/test\n"})}),"\n",(0,t.jsx)(n.p,{children:"This simulates new unseen data arriving \u2014 you train on everything up to those rows, then predict the last 5."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport numpy as np\n\nprint("R2:", round(r2_score(y_test, y_pred), 3))\nprint("MAE:", round(mean_absolute_error(y_test, y_pred), 3))\nprint("RMSE:", round(np.sqrt(mean_squared_error(y_test, y_pred)), 3))\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"R\xb2"})," tells you how much of the price variance your model explains. 0.85 means 85% explained \u2014 the higher the better."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MAE"})," tells you the average error in the same unit as your target (e.g., dollars). Easy to interpret."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RMSE"})," penalises large errors more heavily than MAE. If RMSE is much higher than MAE, your model has a few big misses worth investigating."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"used-car-prices--comparing-multiple-models",children:"Used Car Prices \u2014 Comparing Multiple Models"}),"\n",(0,t.jsx)(n.p,{children:"Same regression setup but this problem goes a step further: it trains four models on the same data and compares them."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Feature engineering the target-related column:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"df['Car_Age'] = 2024 - df['Year']\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The raw ",(0,t.jsx)(n.code,{children:"Year"})," column (e.g., 2015, 2018) is less meaningful to a model than how old the car actually is. Always ask: is the raw value what matters, or is a derived version more informative?"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Comparing models side by side:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"models = {\n    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42),\n    'KNN':           KNeighborsRegressor(n_neighbors=7),\n    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n    'SVR':           SVR(kernel='rbf', C=100, gamma=0.1)\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(f\"{name} \u2192 R2: {r2_score(y_test, y_pred):.3f} | \"\n          f\"MAE: {mean_absolute_error(y_test, y_pred):.3f} | \"\n          f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}\")\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Watch out:"})," KNN and SVR require scaled features \u2014 they use distance calculations. Random Forest and Decision Tree do not require scaling. In this problem, scaling is applied to all models before any of them are trained, so all four get the same prepared data."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"classification-scenarios",children:"Classification Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"customer-churn--binary-classification",children:"Customer Churn \u2014 Binary Classification"}),"\n",(0,t.jsxs)(n.p,{children:["Target: ",(0,t.jsx)(n.code,{children:"Churn"})," \u2014 Yes or No. Two classes."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Encoding the target:"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"Churn"})," column contains string values (",(0,t.jsx)(n.code,{children:"'Yes'"}),"/",(0,t.jsx)(n.code,{children:"'No'"}),"). Most classifiers accept string targets directly, but being explicit is better:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Encoding the features \u2014 LabelEncoder for tree-based models:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"catg_cols = ['Gender', 'ContractType', 'InternetService', 'PaymentMethod']\nencoder = LabelEncoder()\nfor col in catg_cols:\n    df[col] = encoder.fit_transform(df[col])\n"})}),"\n",(0,t.jsxs)(n.p,{children:["LabelEncoder assigns integers (0, 1, 2...) to categories. This is fine for Random Forest because trees split on thresholds \u2014 the numeric ordering doesn't imply a false relationship. For linear or distance-based models, use ",(0,t.jsx)(n.code,{children:"pd.get_dummies()"})," instead."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Scale after reserving prediction rows:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"df_pred = df.tail(3).copy()   # reserve before scaling\ndf = df.iloc[:-3].copy()\n\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\ndf_pred[num_cols] = scaler.transform(df_pred[num_cols])  # transform only, not fit\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This is a critical gotcha \u2014 ",(0,t.jsx)(n.code,{children:"fit_transform"})," on training data, ",(0,t.jsx)(n.code,{children:"transform"})," only on prediction/test data. If you fit the scaler again on the prediction rows, you're computing different statistics and the scaling becomes inconsistent."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Evaluation:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from sklearn.metrics import accuracy_score, precision_score, classification_report\n\nprint("Accuracy:", round(accuracy_score(y_test, y_pred), 3))\nprint("Precision:", round(precision_score(y_test, y_pred), 3))\nprint(classification_report(y_test, y_pred))\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For churn, ",(0,t.jsx)(n.strong,{children:"Recall matters more than Precision"})," \u2014 missing a customer who is about to churn (False Negative) is more costly than flagging one who wasn't going to (False Positive). Accuracy alone can mislead you if most customers don't churn."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"student-course-selection--multi-class-classification",children:"Student Course Selection \u2014 Multi-Class Classification"}),"\n",(0,t.jsxs)(n.p,{children:["Target: ",(0,t.jsx)(n.code,{children:"elective"})," \u2014 one of six courses (DataScience, WebDev, AI, CloudComputing, Cybersecurity, GameDev)."]}),"\n",(0,t.jsx)(n.p,{children:"This is where the encoding approach diverges from the binary case."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"One-hot encoding both features AND target:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"X_en = pd.get_dummies(X, drop_first=True)   # features: drop_first to avoid multicollinearity\ny_en = pd.get_dummies(y)                     # target: do NOT drop_first \u2014 you need all class columns\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Note: ",(0,t.jsx)(n.code,{children:"drop_first=True"})," on the features, but NOT on the target. If you drop one class from the target, you can't recover which course was predicted."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Aligning prediction rows to training columns:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"test_en = pd.get_dummies(test, drop_first=True)\ntest_en = test_en.reindex(columns=X_en.columns, fill_value=0)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["When you one-hot encode a single prediction row, it may not contain all the categories that appeared in training \u2014 so some columns will be missing. ",(0,t.jsx)(n.code,{children:"reindex"})," fills them with 0, ensuring the prediction row has exactly the same shape as what the model was trained on."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Decoding the prediction:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pred = model.predict(test_en)\npred_course = y_en.columns[pred.argmax()]\nprint(pred_course)  # e.g., 'DataScience'\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"predict()"})," here returns a one-hot style array \u2014 ",(0,t.jsx)(n.code,{children:"argmax()"})," finds the column (class) with the highest value, and ",(0,t.jsx)(n.code,{children:"y_en.columns"})," maps it back to the course name."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-gotchas-summary",children:"Key Gotchas Summary"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Situation"}),(0,t.jsx)(n.th,{children:"What to Watch Out For"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Binary target (Yes/No)"}),(0,t.jsx)(n.td,{children:"Map to 1/0 explicitly \u2014 don't rely on alphabetical encoding"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Multi-class target"}),(0,t.jsxs)(n.td,{children:["One-hot encode without ",(0,t.jsx)(n.code,{children:"drop_first"})," \u2014 you need every class column"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"LabelEncoder on features"}),(0,t.jsx)(n.td,{children:"Fine for tree models, not for linear/distance models"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Scaling with reserved prediction rows"}),(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.code,{children:"fit_transform"})," on train data, ",(0,t.jsx)(n.code,{children:"transform"})," only on prediction rows"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Prediction row has unseen categories"}),(0,t.jsxs)(n.td,{children:["Use ",(0,t.jsx)(n.code,{children:"reindex(columns=X_train.columns, fill_value=0)"})," after get_dummies"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Accuracy on imbalanced classes"}),(0,t.jsxs)(n.td,{children:["Use ",(0,t.jsx)(n.code,{children:"classification_report()"})," \u2014 read per-class Precision, Recall, F1"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Outliers before regression"}),(0,t.jsx)(n.td,{children:"Check with IQR before scaling \u2014 one extreme value skews the whole scaler"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Feature engineering"}),(0,t.jsx)(n.td,{children:"Derive meaningful columns before training (e.g., Car_Age = 2024 - Year)"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}},805(e){e.exports=JSON.parse('{"permalink":"/2025/11/13/ml-classification-vs-regression","source":"@site/blog/2025-11-13-ml-classification-vs-regression.md","title":"Machine Learning Practice Problems: Classification vs Regression","description":"A practical walkthrough of ML problems with both categorical and numerical targets \u2014 what changes in your pipeline, how to encode correctly, which metrics to use, and the gotchas to watch out for.","date":"2025-11-13T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"machine learning","permalink":"/tags/machine-learning"},{"inline":true,"label":"learning","permalink":"/tags/learning"}],"readingTime":5.92,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Machine Learning Practice Problems: Classification vs Regression","description":"A practical walkthrough of ML problems with both categorical and numerical targets \u2014 what changes in your pipeline, how to encode correctly, which metrics to use, and the gotchas to watch out for.","tags":["python","machine learning","learning"]},"unlisted":false,"prevItem":{"title":"Understanding AI Agents","permalink":"/2026/01/10/aiagents"},"nextItem":{"title":"Supervised Learning: A Quick Reference","permalink":"/2025/10/28/supervised-learning"}}')}}]);