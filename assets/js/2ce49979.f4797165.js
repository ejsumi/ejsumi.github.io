"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[6103],{8227(e,t,n){n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var s=n(6995),a=n(4848),r=n(8453);const o={title:"RAG from the Inside: What Building It Taught Me About AI-Readable Docs",description:"I built a RAG POC not to ship a product, but to understand the system my documents feed into \u2014 and what it means to write content that AI can actually use.",tags:["python","Gen AI","RAG","LangChain","personal project"]},i=void 0,h={authorsImageUrls:[]},l=[{value:"What RAG Actually Is",id:"what-rag-actually-is",level:2},{value:"Why LangChain and ChromaDB",id:"why-langchain-and-chromadb",level:2},{value:"What I Actually Built",id:"what-i-actually-built",level:2},{value:"Tuning It \u2014 What Actually Changes Responses",id:"tuning-it--what-actually-changes-responses",level:2},{value:"The Bottom Line",id:"the-bottom-line",level:2}];function c(e){const t={code:"code",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"The ask for technical writers team is to make product documentation both human-readable and AI-readable. The idea is that if a RAG-based system is already being used in the knowledge portal to answer user questions, the quality of those answers depends entirely on how well-structured the source documents are. To understand on to make documents AI scalable, I need to actually understand how RAG works from the inside \u2014 not just conceptually, but what happens when you feed it a document and ask a question."}),"\n",(0,a.jsx)(t.p,{children:"So I built a POC. I learned a lot more than I expected."}),"\n",(0,a.jsx)(t.h2,{id:"what-rag-actually-is",children:"What RAG Actually Is"}),"\n",(0,a.jsx)(t.p,{children:"The short version: RAG lets you ask questions about documents that you upload."}),"\n",(0,a.jsx)(t.p,{children:"Here's the thing about regular ChatGPT or any LLM \u2014 it only knows what it was trained on. Ask it about your internal policy document or a PDF from last week, and it either makes something up or tells you it doesn't know."}),"\n",(0,a.jsx)(t.p,{children:"RAG fixes this. Not by retraining the model, but by retrieving the relevant parts of your document and feeding them to the model alongside your question. The model then generates an answer based on what you just handed it, not from memory."}),"\n",(0,a.jsx)(t.p,{children:"There are two distinct phases:"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Indexing"})," \u2014 happens once when you add documents:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"PDF \u2192 Extract Text \u2192 Chunk \u2192 Embed \u2192 Store in VectorDB\n"})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Querying"})," \u2014 happens every time someone asks a question:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"Question \u2192 Embed \u2192 Search VectorDB \u2192 Retrieve Chunks \u2192 LLM \u2192 Answer\n"})}),"\n",(0,a.jsx)(t.p,{children:'The key insight that changed how I think about it: the LLM never "reads" your PDF. It only sees the handful of chunks that were retrieved as most relevant to the question. If the retrieval is off, the answer will be off \u2014 no matter how capable the model is.'}),"\n",(0,a.jsx)(t.h2,{id:"why-langchain-and-chromadb",children:"Why LangChain and ChromaDB"}),"\n",(0,a.jsx)(t.p,{children:"I had a few options for how to build this. I went with LangChain as the orchestration layer and ChromaDB as the vector store, and the reason was simple: I wanted to get something working first and understand the concepts. Also, was doing a course on Langchain and RAG, so easier to go with what I was learning."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"What LangChain is:"})," It's an open-source framework for building LLM-powered applications. Instead of writing all the plumbing yourself \u2014 API calls, prompt construction, memory management, retrieval logic \u2014 LangChain gives you composable building blocks. For this POC, the components I used were:"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"ChatOpenAI"})," \u2014 wrapper around OpenAI's chat model, handles API calls and response parsing"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"ConversationalRetrievalChain"})," \u2014 the core chain that ties everything together: takes a question, retrieves relevant chunks, builds the prompt, calls the LLM, and returns the answer"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"ConversationBufferMemory"})," \u2014 stores the chat history so follow-up questions work without re-asking context"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"PromptTemplate"})," \u2014 structures exactly what gets sent to the LLM, including how context and question are combined"]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Retriever"})," \u2014 the interface that queries ChromaDB and returns the top K matching chunks"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"ChromaDB runs locally, requires no server setup, and stores vectors on disk. Together, they let you go from zero to a working Q&A system in a few hours."}),"\n",(0,a.jsx)(t.p,{children:"The tradeoff is real though. LangChain is a thick abstraction layer. You call high-level functions without fully seeing what's happening underneath \u2014 how the HTTP calls to the embedding API work, exactly how the similarity search runs, how context is being assembled before it hits the LLM. It works, but it hides the internals."}),"\n",(0,a.jsx)(t.h2,{id:"what-i-actually-built",children:"What I Actually Built"}),"\n",(0,a.jsx)(t.p,{children:"The POC is a Streamlit app that:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Accepts a PDF upload"}),"\n",(0,a.jsx)(t.li,{children:"Chunks and embeds the text using OpenAI's embedding model"}),"\n",(0,a.jsx)(t.li,{children:"Stores the vectors in ChromaDB"}),"\n",(0,a.jsx)(t.li,{children:"Accepts questions in a chat interface"}),"\n",(0,a.jsx)(t.li,{children:"Retrieves the top K relevant chunks and sends them to the LLM"}),"\n",(0,a.jsx)(t.li,{children:"Returns an answer with source citations showing which parts of the document it drew from"}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Three components, each independently tuneable:"}),"\n",(0,a.jsxs)(t.table,{children:[(0,a.jsx)(t.thead,{children:(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.th,{children:"Component"}),(0,a.jsx)(t.th,{children:"What It Does"}),(0,a.jsx)(t.th,{children:"What I Used"})]})}),(0,a.jsxs)(t.tbody,{children:[(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"PDF Processor"}),(0,a.jsx)(t.td,{children:"Extracts and chunks the text"}),(0,a.jsx)(t.td,{children:"pypdf + LangChain text splitter"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"Embedding Model"}),(0,a.jsx)(t.td,{children:"Converts chunks to vectors for search"}),(0,a.jsx)(t.td,{children:"text-embedding-3-small"})]}),(0,a.jsxs)(t.tr,{children:[(0,a.jsx)(t.td,{children:"LLM"}),(0,a.jsx)(t.td,{children:"Generates answers from retrieved chunks"}),(0,a.jsx)(t.td,{children:"gpt-4o-mini"})]})]})]}),"\n",(0,a.jsx)(t.h2,{id:"tuning-it--what-actually-changes-responses",children:"Tuning It \u2014 What Actually Changes Responses"}),"\n",(0,a.jsx)(t.p,{children:"This is where it got interesting."}),"\n",(0,a.jsx)(t.p,{children:"The first answers I got were technically correct but flat. Too literal. The model was pulling exact phrases from the document and reciting them without much synthesis. I started tuning."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The prompt template"})," made the biggest difference. Changing how I framed the task \u2014 what tone to use, how to handle uncertainty, whether to synthesize across chunks or quote directly \u2014 changed the response quality significantly. This is free to change and has no latency cost."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Temperature"})," was the one that surprised me most. At low temperature (around 0.1\u20130.2), the model becomes very conservative. It sticks close to exact wording in the retrieved chunks. If the question uses slightly different phrasing than the document, it can fail to connect them \u2014 even when a human would obviously see the relationship. It's not that the answer is wrong; it's that it's too literal."]}),"\n",(0,a.jsx)(t.p,{children:"Raising the temperature to around 0.5\u20130.7 made the responses more natural and better at bridging paraphrase gaps. The model was more willing to interpret rather than just recite. For factual Q&A, somewhere in the 0.3\u20130.5 range felt like the right balance \u2014 enough flexibility to handle varied phrasing, not so much that it starts drifting from what's actually in the document."}),"\n",(0,a.jsx)(t.p,{children:"The other parameters worth knowing:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"RETRIEVAL_K"})," \u2014 how many chunks get passed to the LLM. More chunks = more context. Going from 3 to 6 noticeably improved answers on complex questions where context was spread across the document."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"CHUNK_SIZE / CHUNK_OVERLAP"})," \u2014 smaller chunks give more precise retrieval but can miss context that spans paragraphs. Larger chunks capture more context but can bring in irrelevant content. Changing these requires re-indexing everything."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"the-bottom-line",children:"The Bottom Line"}),"\n",(0,a.jsx)(t.p,{children:"RAG is conceptually simple. Two phases, three components. The hard part is understanding which component to adjust when the answers aren't good enough \u2014 and right now, the fastest levers are the prompt template, retrival chunks and temperature."}),"\n",(0,a.jsx)(t.p,{children:"LangChain and ChromaDB are the right tools to start with. They get you to a working system quickly so you can focus on what actually matters: the quality of retrieval and the quality of the prompts."}),"\n",(0,a.jsx)(t.p,{children:"Once you've got that, you peel back the abstraction and build it the hard way. That's where the real understanding is. Next up: rebuilding this without LangChain \u2014 raw API calls, custom chunking, manual prompt construction \u2014 to see exactly what the framework was doing for me."}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.em,{children:"Built with Python, LangChain, ChromaDB, and OpenAI. Running locally with Streamlit."})})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453(e,t,n){n.d(t,{R:()=>o,x:()=>i});var s=n(6540);const a={},r=s.createContext(a);function o(e){const t=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:t},e.children)}},6995(e){e.exports=JSON.parse('{"permalink":"/2026/02/14/rag-poc","source":"@site/blog/2026-02-14-rag-poc.md","title":"RAG from the Inside: What Building It Taught Me About AI-Readable Docs","description":"I built a RAG POC not to ship a product, but to understand the system my documents feed into \u2014 and what it means to write content that AI can actually use.","date":"2026-02-14T00:00:00.000Z","tags":[{"inline":true,"label":"python","permalink":"/tags/python"},{"inline":true,"label":"Gen AI","permalink":"/tags/gen-ai"},{"inline":true,"label":"RAG","permalink":"/tags/rag"},{"inline":true,"label":"LangChain","permalink":"/tags/lang-chain"},{"inline":true,"label":"personal project","permalink":"/tags/personal-project"}],"readingTime":5.37,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"RAG from the Inside: What Building It Taught Me About AI-Readable Docs","description":"I built a RAG POC not to ship a product, but to understand the system my documents feed into \u2014 and what it means to write content that AI can actually use.","tags":["python","Gen AI","RAG","LangChain","personal project"]},"unlisted":false,"nextItem":{"title":"From Jekyll to Docusaurus","permalink":"/2026/01/25/jekyll-to-docusaurus"}}')}}]);