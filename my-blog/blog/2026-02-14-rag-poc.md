---
title: "RAG from the Inside: What Building It Taught Me About AI-Readable Docs"
description: "I built a RAG POC not to ship a product, but to understand the system my documents feed into — and what it means to write content that AI can actually use."
tags: [python, Gen AI, RAG, LangChain, personal project]
---

The ask for technical writers team is to make product documentation both human-readable and AI-readable. The idea is that if a RAG-based system is already being used in the knowledge portal to answer user questions, the quality of those answers depends entirely on how well-structured the source documents are. To understand on to make documents AI scalable, I need to actually understand how RAG works from the inside — not just conceptually, but what happens when you feed it a document and ask a question.

So I built a POC. I learned a lot more than I expected.

## What RAG Actually Is

The short version: RAG lets you ask questions about documents that you upload.

Here's the thing about regular ChatGPT or any LLM — it only knows what it was trained on. Ask it about your internal policy document or a PDF from last week, and it either makes something up or tells you it doesn't know.

RAG fixes this. Not by retraining the model, but by retrieving the relevant parts of your document and feeding them to the model alongside your question. The model then generates an answer based on what you just handed it, not from memory.

There are two distinct phases:

**Indexing** — happens once when you add documents:

```
PDF → Extract Text → Chunk → Embed → Store in VectorDB
```

**Querying** — happens every time someone asks a question:

```
Question → Embed → Search VectorDB → Retrieve Chunks → LLM → Answer
```

The key insight that changed how I think about it: the LLM never "reads" your PDF. It only sees the handful of chunks that were retrieved as most relevant to the question. If the retrieval is off, the answer will be off — no matter how capable the model is.

## Why LangChain and ChromaDB

I had a few options for how to build this. I went with LangChain as the orchestration layer and ChromaDB as the vector store, and the reason was simple: I wanted to get something working first and understand the concepts. Also, was doing a course on Langchain and RAG, so easier to go with what I was learning.

**What LangChain is:** It's an open-source framework for building LLM-powered applications. Instead of writing all the plumbing yourself — API calls, prompt construction, memory management, retrieval logic — LangChain gives you composable building blocks. For this POC, the components I used were:

- **ChatOpenAI** — wrapper around OpenAI's chat model, handles API calls and response parsing
- **ConversationalRetrievalChain** — the core chain that ties everything together: takes a question, retrieves relevant chunks, builds the prompt, calls the LLM, and returns the answer
- **ConversationBufferMemory** — stores the chat history so follow-up questions work without re-asking context
- **PromptTemplate** — structures exactly what gets sent to the LLM, including how context and question are combined
- **Retriever** — the interface that queries ChromaDB and returns the top K matching chunks

ChromaDB runs locally, requires no server setup, and stores vectors on disk. Together, they let you go from zero to a working Q&A system in a few hours.

The tradeoff is real though. LangChain is a thick abstraction layer. You call high-level functions without fully seeing what's happening underneath — how the HTTP calls to the embedding API work, exactly how the similarity search runs, how context is being assembled before it hits the LLM. It works, but it hides the internals.

## What I Actually Built

The POC is a Streamlit app that:
- Accepts a PDF upload
- Chunks and embeds the text using OpenAI's embedding model
- Stores the vectors in ChromaDB
- Accepts questions in a chat interface
- Retrieves the top K relevant chunks and sends them to the LLM
- Returns an answer with source citations showing which parts of the document it drew from

Three components, each independently tuneable:

| Component | What It Does | What I Used |
|-----------|-------------|-------------|
| PDF Processor | Extracts and chunks the text | pypdf + LangChain text splitter |
| Embedding Model | Converts chunks to vectors for search | text-embedding-3-small |
| LLM | Generates answers from retrieved chunks | gpt-4o-mini |

## Tuning It — What Actually Changes Responses

This is where it got interesting.

The first answers I got were technically correct but flat. Too literal. The model was pulling exact phrases from the document and reciting them without much synthesis. I started tuning.

**The prompt template** made the biggest difference. Changing how I framed the task — what tone to use, how to handle uncertainty, whether to synthesize across chunks or quote directly — changed the response quality significantly. This is free to change and has no latency cost.

**Temperature** was the one that surprised me most. At low temperature (around 0.1–0.2), the model becomes very conservative. It sticks close to exact wording in the retrieved chunks. If the question uses slightly different phrasing than the document, it can fail to connect them — even when a human would obviously see the relationship. It's not that the answer is wrong; it's that it's too literal.

Raising the temperature to around 0.5–0.7 made the responses more natural and better at bridging paraphrase gaps. The model was more willing to interpret rather than just recite. For factual Q&A, somewhere in the 0.3–0.5 range felt like the right balance — enough flexibility to handle varied phrasing, not so much that it starts drifting from what's actually in the document.

The other parameters worth knowing:

- **RETRIEVAL_K** — how many chunks get passed to the LLM. More chunks = more context. Going from 3 to 6 noticeably improved answers on complex questions where context was spread across the document.
- **CHUNK_SIZE / CHUNK_OVERLAP** — smaller chunks give more precise retrieval but can miss context that spans paragraphs. Larger chunks capture more context but can bring in irrelevant content. Changing these requires re-indexing everything.

## The Bottom Line

RAG is conceptually simple. Two phases, three components. The hard part is understanding which component to adjust when the answers aren't good enough — and right now, the fastest levers are the prompt template, retrival chunks and temperature.

LangChain and ChromaDB are the right tools to start with. They get you to a working system quickly so you can focus on what actually matters: the quality of retrieval and the quality of the prompts.

Once you've got that, you peel back the abstraction and build it the hard way. That's where the real understanding is. Next up: rebuilding this without LangChain — raw API calls, custom chunking, manual prompt construction — to see exactly what the framework was doing for me.

---

*Built with Python, LangChain, ChromaDB, and OpenAI. Running locally with Streamlit.*
